# -*- coding: utf-8 -*-
"""POS Tagger.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p7tIWCIA3KNnCP41s-QAEcCni_Cimyfa
"""

!pip install transformers
!pip install datasets

import os
from transformers import XLMRobertaTokenizerFast, XLMRobertaForTokenClassification, TrainingArguments, Trainer, EarlyStoppingCallback
from datasets import Dataset
from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/drive')

def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.read().strip().split('\n')

    sentences, tags = [], []
    sentence, tag_seq = [], []

    for line in lines:
        try:
            if line.strip():
                word, tag = line.split()
                sentence.append(word)
                tag_seq.append(tag)

                # Check for full stop (FS) to mark the end of a sentence
                if tag == 'FS':
                    sentences.append(sentence)
                    tags.append(tag_seq)
                    sentence, tag_seq = [], []
        except Exception as e:
            print(f"Error processing line: {line}")
            print(e)

    # Add the last sentence if not added
    if sentence:
        sentences.append(sentence)
        tags.append(tag_seq)

    return sentences, tags

file_path = "/content/drive/MyDrive/AI Project/Assets/news- verified- final level.txt"  # Adjust the file path as needed
sentences, tags = load_data(file_path)

print(sentences[:5])
print(tags[:5])
print(len(sentences))
print(len(tags))

# Create a tokenizer and label mapping
tokenizer = XLMRobertaTokenizerFast.from_pretrained("xlm-roberta-large")

# Create a unique label set and a mapping
unique_tags = list(set(tag for tag_seq in tags for tag in tag_seq))
label2id = {tag: idx for idx, tag in enumerate(unique_tags)}
id2label = {idx: tag for tag, idx in label2id.items()}

def encode_examples(sentences, tags, tokenizer, label2id, max_length=128):
    encoded_data = []
    for sent, tag_seq in zip(sentences, tags):
        tokens = tokenizer(sent, is_split_into_words=True, truncation=True, max_length=max_length, padding="max_length")
        word_ids = tokens.word_ids()
        labels = [-100 if idx is None else label2id[tag_seq[idx]] for idx in word_ids]
        tokens["labels"] = labels
        encoded_data.append(tokens)
    return encoded_data

encoded_data = encode_examples(sentences, tags, tokenizer, label2id)

print(encoded_data[:5])

# Prepare Dataset
train_data, test_data = train_test_split(encoded_data, test_size=0.2, random_state=42)
train_dataset = Dataset.from_dict({key: [x[key] for x in train_data] for key in train_data[0]})
test_dataset = Dataset.from_dict({key: [x[key] for x in test_data] for key in test_data[0]})

model_save_path = "/content/drive/MyDrive/AI Project/Assets/final_model"

# Load pre-trained model or initialize a new one
if os.path.exists(model_save_path):
    print("Loading pre-trained model and tokenizer...")
    model = XLMRobertaForTokenClassification.from_pretrained(model_save_path)
    tokenizer = XLMRobertaTokenizerFast.from_pretrained(model_save_path)
else:
    print("Initializing a new model...")
    model = XLMRobertaForTokenClassification.from_pretrained("xlm-roberta-large", num_labels=len(unique_tags))
    model.config.id2label = id2label
    model.config.label2id = label2id

train = False

# Load Pre-trained Model and Fine-tune
if train:
    training_args = TrainingArguments(
        output_dir="./results",
        evaluation_strategy="epoch",
        learning_rate=2e-5,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        num_train_epochs=10,  # Higher epochs for early stopping
        weight_decay=0.01,
        save_total_limit=2,
        save_steps=10,
        logging_dir='./logs',
        logging_steps=10,
        load_best_model_at_end=True  # Load the best model after training
    )

    # Early stopping callback
    early_stopping_callback = EarlyStoppingCallback(
        early_stopping_patience=3,  # Stop if no improvement for 3 evaluation steps
        early_stopping_threshold=0.01  # Minimum improvement threshold
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        tokenizer=tokenizer,
        callbacks=[early_stopping_callback]
    )

    # Train the model
    print("Starting training...")
    trainer.train()

    # Save the trained model and tokenizer
    trainer.save_model(model_save_path)
    tokenizer.save_pretrained(model_save_path)

    print(f"Model and tokenizer saved to {model_save_path}")
else:
    print("Skipping training as 'train' is set to False.")

import torch

# Ensure the model and input tensors are on the same device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

example_sentence = ["ඊශ්‍රායල්", "මිසයිල", "ප්‍රහාර", "වලින්", "පලස්තීනුවෝ", "සිව්දෙනෙක්", "මිය", "යති", "."]

# Move input tensors to the same device as the model
inputs = tokenizer(example_sentence, is_split_into_words=True, return_tensors="pt", truncation=True, padding=True)
inputs = {key: value.to(device) for key, value in inputs.items()}

# Forward pass
outputs = model(**inputs)
predictions = outputs.logits.argmax(dim=-1).squeeze().tolist()
predicted_tags = [id2label[pred] for pred in predictions if pred != -100]

print("Example Sentence:", example_sentence)
print("Predicted Tags:", predicted_tags)