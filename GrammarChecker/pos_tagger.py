# -*- coding: utf-8 -*-
"""POS Tagger.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p7tIWCIA3KNnCP41s-QAEcCni_Cimyfa
"""

!pip install transformers
!pip install datasets

import os
from transformers import XLMRobertaTokenizerFast, XLMRobertaForTokenClassification, TrainingArguments, Trainer
from datasets import Dataset
from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/drive')

def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.read().strip().split('\n')

    sentences, tags = [], []
    sentence, tag_seq = [], []

    for line in lines:
        try:
            if line.strip():
                word, tag = line.split()
                sentence.append(word)
                tag_seq.append(tag)

                # Check for full stop (FS) to mark the end of a sentence
                if tag == 'FS':
                    sentences.append(sentence)
                    tags.append(tag_seq)
                    sentence, tag_seq = [], []
        except Exception as e:
            print(f"Error processing line: {line}")
            print(e)

    # Add the last sentence if not added
    if sentence:
        sentences.append(sentence)
        tags.append(tag_seq)

    return sentences, tags

file_path = "/content/drive/MyDrive/AI Project/Assets/news- verified- final level.txt"  # Adjust the file path as needed
sentences, tags = load_data(file_path)

print(sentences[:5])
print(tags[:5])
print(len(sentences))
print(len(tags))

# Create a tokenizer and label mapping
tokenizer = XLMRobertaTokenizerFast.from_pretrained("xlm-roberta-large")

# Create a unique label set and a mapping
unique_tags = list(set(tag for tag_seq in tags for tag in tag_seq))
label2id = {tag: idx for idx, tag in enumerate(unique_tags)}
id2label = {idx: tag for tag, idx in label2id.items()}

def encode_examples(sentences, tags, tokenizer, label2id, max_length=128):
    encoded_data = []
    for sent, tag_seq in zip(sentences, tags):
        tokens = tokenizer(sent, is_split_into_words=True, truncation=True, max_length=max_length, padding="max_length")
        word_ids = tokens.word_ids()
        labels = [-100 if idx is None else label2id[tag_seq[idx]] for idx in word_ids]
        tokens["labels"] = labels
        encoded_data.append(tokens)
    return encoded_data

encoded_data = encode_examples(sentences, tags, tokenizer, label2id)

print(encoded_data[:5])

# Prepare Dataset
train_data, test_data = train_test_split(encoded_data, test_size=0.2, random_state=42)
train_dataset = Dataset.from_dict({key: [x[key] for x in train_data] for key in train_data[0]})
test_dataset = Dataset.from_dict({key: [x[key] for x in test_data] for key in test_data[0]})

# Prepare Dataset
train_data, test_data = train_test_split(encoded_data, test_size=0.2, random_state=42)
train_dataset = Dataset.from_dict({key: [x[key] for x in train_data] for key in train_data[0]})
test_dataset = Dataset.from_dict({key: [x[key] for x in test_data] for key in test_data[0]})

# Load Pre-trained Model and Fine-tune
model = XLMRobertaForTokenClassification.from_pretrained("xlm-roberta-large", num_labels=len(unique_tags))
model.config.id2label = id2label
model.config.label2id = label2id

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=2,
    save_steps=10,
    logging_dir='./logs',
    logging_steps=10
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer
)

# Train the model
trainer.train()

import torch

# Ensure the model and input tensors are on the same device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

example_sentence = ["ඊශ්‍රායල්", "මිසයිල", "ප්‍රහාර", "වලින්", "පලස්තීනුවෝ"]

# Move input tensors to the same device as the model
inputs = tokenizer(example_sentence, is_split_into_words=True, return_tensors="pt", truncation=True, padding=True)
inputs = {key: value.to(device) for key, value in inputs.items()}

# Forward pass
outputs = model(**inputs)
predictions = outputs.logits.argmax(dim=-1).squeeze().tolist()
predicted_tags = [id2label[pred] for pred in predictions if pred != -100]

print("Example Sentence:", example_sentence)
print("Predicted Tags:", predicted_tags)

# Save the model and tokenizer
model_save_path = "/content/drive/MyDrive/AI Project/Assets/final_model"

# Save the trained model
trainer.save_model(model_save_path)

# Save the tokenizer
tokenizer.save_pretrained(model_save_path)

print(f"Model and tokenizer saved to {model_save_path}")